{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LMQL with local models (i.e. HuggingFace, Llama, etc.)\n",
    "Make sure to have followed the setup instructions in the `README.md` and that you have a large enough GPU. This example will use the Mistral 7B models ([Mistra.ai](https://mistral.ai/news/announcing-mistral-7b/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model service endpoint\n",
    "Local models have to be hosted using LMQL ´serve-model´. This is run in the terminal to create an endpoint that is used in the code. Read more here: [LMQL - Local Models / Transformers](https://lmql.ai/docs/models/hf.html)\n",
    "\n",
    "It is possible to add additional parameters that are normally given in the `AutoModelForCausalLM.from_pretrained(...)` HuggingFace function ([see documentation](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoConfig.from_pretrained)). This allows setting parameters such as the `repetition_penalty` or `load_in_4bit` (quantization). To do this, specify the parameter with double dashes and the parameter value after a space (i.e. `--load_in_4bit True`). Use the `port` parameter to specify the port number and add `--cuda` to load the model on GPU.\n",
    "\n",
    "This is a basic example of loading the Mistral 7B Instruct model on GPU with an increased repetition penalty and quantization.\n",
    "\n",
    "Run in the terminal:  \n",
    "```\n",
    "lmql serve-model mistralai/Mistral-7B-Instruct-v0.1 --repetition_penalty 1.1 --load_in_4bit True --cuda --port 9999\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/miniconda3/envs/lmql_demo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lmql\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "llm = lmql.model(model_name, endpoint=\"localhost:9999\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt formats and completion/chat models\n",
    "With local GPTs, it's critical to distinguish between two model types: *completion models* and *chat/instruct models*. This has important implications for the way the model should be prompted. I recommend reading this great article on the difference between the two: [Language Models - Completion and Chat-Completion](https://langroid.github.io/langroid/blog/2023/09/19/language-models-completion-and-chat-completion/)\n",
    "\n",
    "**Prompt formats and special tokens**  \n",
    "Chat models (often called chat, instruct, hf (as in \"human feedback\"), or similar) have been trained using special tokens to teach the model the difference between user input and assistant output. When using a model, it's important to understand what special tokens and prompt format is expected. The Mistral models use a similar input to Llama 2. More details can be found here: [Prompt Engineering Guide - Mistral 7B LLM](https://www.promptingguide.ai/models/mistral-7b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "BINST_TOKEN = \"[INST]\"\n",
    "EINST_TOKEN = \"[/INST]\"\n",
    "\n",
    "emotions_list = [\"happy\", \"sad\", \"dissapointed\", \"angry\", \"frustrated\", \"joyful\", \"anxious\"]\n",
    "\n",
    "@lmql.query(model=llm)\n",
    "def meaning():\n",
    "    '''lmql\n",
    "    argmax\n",
    "        \"{BOS_TOKEN}{BINST_TOKEN} I have stolen my dog's toy. What emotion do you think my dog will feel? {EINST_TOKEN}\"\n",
    "        \n",
    "        \"Let's think about this for a second... [ANALYSIS]\" where not \"\\n\" in ANALYSIS\n",
    "        \n",
    "        \"Based on this analysis, the emotion your dog will most likely experience is: [EMOTION]\" distribution EMOTION in emotions_list\n",
    "    '''\n",
    "\n",
    "result = meaning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>[INST] I have stolen my dog's toy. What emotion do you think my dog will feel? [/INST]Let's think about this for a second...  If you stole your dog's toy, it's likely that your dog would feel emotions such as sadness, frustration or anger. They may also exhibit behaviors such as whining, pacing, or loss of appetite. It's important to remember that dogs form strong emotional bonds with their toys, and taking something they value away can cause distress.Based on this analysis, the emotion your dog will most likely experience is: dissapointed\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " If you stole your dog's toy, it's likely that your dog would feel emotions such as sadness, frustration or anger. They may also exhibit behaviors such as whining, pacing, or loss of appetite. It's important to remember that dogs form strong emotional bonds with their toys, and taking something they value away can cause distress.\n"
     ]
    }
   ],
   "source": [
    "print(result.variables['ANALYSIS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('happy', 0.0005932049404840637), ('sad', 0.0865745248912329), ('dissapointed', 0.6188276273220179), ('angry', 0.07515209122145111), ('frustrated', 0.1612957475629304), ('joyful', 0.0035589946695807942), ('anxious', 0.05399780939230316)]\n"
     ]
    }
   ],
   "source": [
    "print(result.variables['P(EMOTION)'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lmql_demo_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
